{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\GRU_formulas.jpg\" width=800>\n",
    "<img src=\"images\\GRU_shapes.jpg\" width=200>\n",
    "<img src=\"images\\GRU_derivatives.jpg\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"]=16,5\n",
    "\n",
    "\n",
    "def load_data_txt(filename, dimensions, max_len=None):\n",
    "    f = open(filename, \"r\")\n",
    "    data = []\n",
    "    print(\"loading data\")\n",
    "    for line in f:\n",
    "        for c in line:\n",
    "            if ord(c)-ord('A') > 0 and ord(c)-ord('A') < dimensions:\n",
    "                new_data = []\n",
    "                for i in range(0, dimensions):\n",
    "                    new_data.append([0])\n",
    "                    \n",
    "                new_data[ord(c)-ord('A')] = [1]\n",
    "                data.append(new_data)\n",
    "                if max_len != None:\n",
    "                    if len(data) >= max_len:\n",
    "                        print(\"all loaded\")    \n",
    "                        return data\n",
    "                    \n",
    "    print(\"all loaded\")    \n",
    "    return data\n",
    "            \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def swish(x):\n",
    "    return x*sigmoid(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def softmax(x):\n",
    "    p = np.exp(x - np.max(x))\n",
    "    return p/np.sum(p)\n",
    "\n",
    "def MSE(target, x):\n",
    "    return np.power(target - x, 2)\n",
    "\n",
    "def activation_function(z, act, derivative=False, activated_value=None):\n",
    "    if derivative == False:\n",
    "        if act==\"sigmoid\":\n",
    "            return sigmoid(z)\n",
    "        elif act == \"swish\":\n",
    "            return swish(z)\n",
    "        elif act == \"relu\":\n",
    "            return relu(z)\n",
    "        elif act==\"tanh\":\n",
    "            return np.tanh(z)\n",
    "        elif act==\"softmax\":\n",
    "            return softmax(z)\n",
    "    else:\n",
    "        if act==\"sigmoid\":\n",
    "            return activated_value * (1 - activated_value)\n",
    "        elif act == \"swish\":\n",
    "            return activated_value + sigmoid(z) * (1 - activated_value)\n",
    "        elif act == \"relu\":  \n",
    "            x = activated_value\n",
    "            x[x<=0] = 0\n",
    "            x[x>0] = 1\n",
    "            return x\n",
    "        \n",
    "        elif act==\"tanh\":\n",
    "            return 1 - np.power(activated_value, 2)\n",
    "        elif act==\"softmax\":\n",
    "            return activated_value\n",
    "\n",
    "class RNN():\n",
    "    \n",
    "    def __init__(self, l_rate):\n",
    "        self.layers = []\n",
    "        self.learning_rate = l_rate\n",
    "        \n",
    "    def add_layer(self, input_size, output_size, last_activation=None):\n",
    "        new_layer = Layer(input_size, output_size, last_activation)\n",
    "        self.layers.append(new_layer)\n",
    "        \n",
    "    def forward_pass(self, input_data):\n",
    "        self.layers[0].forward_pass(input_data)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            prev_layer_out = self.layers[i-1].h[-1]        # Takes in the output of the previous layer. h[-1] is the hidden state of the last timestep of the layer\n",
    "            self.layers[i].forward_pass(prev_layer_out)\n",
    "            \n",
    "        return self.layers[-1].h[-1]\n",
    "            \n",
    "    def clear_memory(self):\n",
    "        for i in range(0, len(self.layers)):\n",
    "            self.layers[i].clear_memory()\n",
    "            \n",
    "    def backpropagation_through_time(self, input_data, target, max_steps):\n",
    "        gradient, loss = self.layers[-1].der_MSE(target)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            inputs = self.layers[-i-1].h\n",
    "            gradient = self.layers[-i].descent(gradient, inputs, self.learning_rate, max_steps)\n",
    "        self.layers[0].descent(gradient, input_data, self.learning_rate, max_steps)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        next_letter = []\n",
    "        out_letter = []\n",
    "        for i in range(0, len(input_data)):\n",
    "            next_letter = self.forward_pass(input_data[i])\n",
    "            \n",
    "        out_letter = next_letter\n",
    "        print_output = \"\"\n",
    "        for i in range(500, 700):\n",
    "            maximum = 0;\n",
    "            max_index = 0;\n",
    "            for k in range(0, 3):\n",
    "                if next_letter[k][0] > maximum:\n",
    "                    maximum = next_letter[k][0]\n",
    "                    max_index = k\n",
    "            out_letter = np.empty(next_letter.shape)\n",
    "            out_letter.fill(0)\n",
    "            out_letter[max_index,0] = 1\n",
    "            print_output += chr(ord('A')+max_index)\n",
    "            next_letter = self.forward_pass(out_letter)\n",
    "        print(print_output)\n",
    "    \n",
    "class Layer():\n",
    "    \n",
    "    def __init__(self, input_size, output_size, last_activation=None):\n",
    "        self.input_size = input_size # N\n",
    "        self.output_size = output_size # M\n",
    "        self.last_activation = last_activation\n",
    "        \n",
    "        self.Wz = self.weights_init(output_size, input_size) # NxM\n",
    "        self.Wr = self.weights_init(output_size, input_size) # NxM\n",
    "        self.Wh = self.weights_init(output_size, input_size) # NxM\n",
    "        \n",
    "        self.Uz = self.weights_init(output_size, output_size) # NxN\n",
    "        self.Ur = self.weights_init(output_size, output_size) # NxN\n",
    "        self.Uh = self.weights_init(output_size, output_size) # NxN\n",
    "        \n",
    "        self.bz = self.weights_init(output_size, 1, bias=True) # Nx1\n",
    "        self.br = self.weights_init(output_size, 1, bias=True) # Nx1\n",
    "        self.bh = self.weights_init(output_size, 1, bias=True) # Nx1\n",
    "        \n",
    "        # T = time steps, starts with 0\n",
    "        \n",
    "        self.z       = np.empty((0,output_size,1)) # TxNx1, z(t) = Nx1\n",
    "        self.r       = np.empty((0,output_size,1)) # TxNx1, r(t) = Nx1\n",
    "        self.h_tilde = np.empty((0,output_size,1)) # TxNx1, h_tilde(t) = Nx1\n",
    "        self.h       = np.empty((0,output_size,1)) # TxNx1, h(t) = Nx1\n",
    "        self.inputs  = np.empty((0,input_size,1))  # TxMx1, inputs(t) = Mx1\n",
    "        \n",
    "        self.v_Wy = 0\n",
    "        self.v_Wh = 0\n",
    "        self.v_B = 0\n",
    "        self.v_Wx = 0\n",
    "        \n",
    "    def weights_init(self, rows, cols, bias=False):\n",
    "        variance = math.sqrt(1/self.input_size)\n",
    "        if bias:\n",
    "            return np.random.uniform(0,0.01,(rows, cols))\n",
    "        return np.random.uniform(-variance,variance,(rows, cols))\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, input_data):\n",
    "        \n",
    "        self.inputs = np.append(self.inputs, [input_data])\n",
    "        prev_h = []\n",
    "        if len(self.h) > 0:\n",
    "            prev_h = self.h[-1]\n",
    "        else:\n",
    "            prev_h = np.empty((self.h.shape[1], self.h.shape[2]))\n",
    "            prev_h.fill(0)\n",
    "        \n",
    "        new_z = self.Wz @ input_data + self.Uz @ prev_h + self.bz\n",
    "        new_z = activation_function(new_z, \"sigmoid\")\n",
    "        self.z = np.append(self.z, [new_z], axis=0)\n",
    "        \n",
    "        new_r = self.Wr @ input_data + self.Ur @ prev_h + self.br\n",
    "        new_r = activation_function(new_r, \"sigmoid\")\n",
    "        self.r = np.append(self.r, [new_r], axis=0)\n",
    "        \n",
    "        new_h_tilde = self.Wh @ input_data + self.r[-1] * (self.Uh @ prev_h) + self.bh\n",
    "        new_h_tilde = activation_function(new_h_tilde, \"tanh\")\n",
    "        self.h_tilde = np.append(self.h_tilde, [new_h_tilde], axis=0)\n",
    "        \n",
    "        new_h = self.z[-1] * self.h_tilde[-1] + (1 - self.z[-1]) * prev_h\n",
    "        self.h = np.append(self.h, [new_h], axis=0)\n",
    "        \n",
    "        \n",
    "        if len(self.z) > 30:\n",
    "            self.z = np.delete(self.z, 0, 0)\n",
    "        if len(self.r) > 30:\n",
    "            self.r = np.delete(self.r, 0, 0)\n",
    "        if len(self.h_tilde) > 30:\n",
    "            self.h_tilde = np.delete(self.h_tilde, 0, 0)\n",
    "        if len(self.h) > 30:\n",
    "            self.h = np.delete(self.h, 0, 0)\n",
    "            \n",
    "            \n",
    "        \n",
    "    def clear_memory(self):\n",
    "        self.z       = np.empty((0,self.output_size,1))\n",
    "        self.r       = np.empty((0,self.output_size,1))\n",
    "        self.h_tilde = np.empty((0,self.output_size,1))\n",
    "        self.h       = np.empty((0,self.output_size,1))\n",
    "        \n",
    "        self.v_Wy = 0\n",
    "        self.v_Wh = 0\n",
    "        self.v_B = 0\n",
    "        self.v_Wx = 0\n",
    "        \n",
    "        \n",
    "    def der_MSE(self, target):\n",
    "        if self.last_activation != None:\n",
    "            return 2*(self.h[-1] - target) * activation_function(self.h[-1], self.last_activation, derivative=True, activated_value=self.h[-1]), np.power(target - self.h[-1], 2)\n",
    "        return 2*(self.h[-1] - target), np.power(target - self.h[-1], 2)\n",
    "     \n",
    "    \n",
    "    def descent(self, gradient, input_data, learning_rate, max_steps):\n",
    "        input_matrix = np.array(input_data)\n",
    "        first_index = max(len(self.h) - max_steps, 0)\n",
    "        \n",
    "        step = 0\n",
    "        inputs_reshaped = np.sum(self.inputs[first_index], axis=0, keepdims=True)\n",
    "        inputs_reshaped = np.tile(inputs_reshaped, (self.output_size, 1))\n",
    "        \n",
    "        \n",
    "        dr_Wr = activation_function(self.r[first_index], \"sigmoid\", derivative=True, activated_value=self.r[first_index])\n",
    "        dr_Wr *= (inputs_reshaped)\n",
    "        #dr_Wr = np.tile(dr_Wr, (1, self.input_size))\n",
    "        \n",
    "        dz_Wr = activation_function(self.z[first_index], \"sigmoid\", derivative=True, activated_value=self.z[first_index])\n",
    "        dz_Wr *= (inputs_reshaped)\n",
    "        \n",
    "        \n",
    "        dhtilde_Wr = activation_function(self.h_tilde[first_index], \"tanh\", derivative=True, activated_value=self.h_tilde[first_index])\n",
    "        dhtilde_Wr *= (inputs_reshaped)\n",
    "        \n",
    "        dh_Wr = dz_Wr * (-self.h_tilde[first_index]) + dhtilde_Wr * (1 - self.z[first_index])\n",
    "        \n",
    "        \n",
    "        \n",
    "        dr_Wz = dr_Wr\n",
    "        #dr_Wr = np.tile(dr_Wr, (1, self.input_size))\n",
    "        dz_Wz = dz_Wr\n",
    "        \n",
    "        dhtilde_Wz = dhtilde_Wr\n",
    "        \n",
    "        dh_Wz = dz_Wz * (-self.h_tilde[first_index]) + dhtilde_Wz * (1 - self.z[first_index])\n",
    "        \n",
    "        \n",
    "        \n",
    "        dz_Uz = activation_function(self.z[first_index], \"sigmoid\", derivative=True, activated_value=self.z[first_index])\n",
    "        dr_Uz = activation_function(self.r[first_index], \"sigmoid\", derivative=True, activated_value=self.r[first_index])\n",
    "        dhtilde_Uz = activation_function(self.h_tilde[first_index], \"tanh\", derivative=True, activated_value=self.h_tilde[first_index])\n",
    "        dh_Uz = dz_Uz * (-self.h_tilde[first_index]) + dhtilde_Uz * (1 - self.z[first_index])\n",
    "        \n",
    "        dr_x = activation_function(self.r[first_index], \"sigmoid\", derivative=True, activated_value=self.r[first_index])\n",
    "        dr_x = np.tile(dr_x, (1, self.input_size))\n",
    "        dr_x = np.sum(dr_x, axis=0, keepdims=True).T\n",
    "        Wr_reshaped = np.sum(self.Wr, axis=0, keepdims=True).T\n",
    "        dr_x *= (Wr_reshaped)\n",
    "        \n",
    "        dz_x = activation_function(self.z[first_index], \"sigmoid\", derivative=True, activated_value=self.z[first_index])\n",
    "        dz_x = np.tile(dz_x, (1, self.input_size))\n",
    "        dz_x = np.sum(dz_x, axis=0, keepdims=True).T\n",
    "        Wz_reshaped = np.sum(self.Wz, axis=0, keepdims=True).T\n",
    "        dz_x *= (Wz_reshaped)\n",
    "        \n",
    "        dhtilde_x = activation_function(self.h_tilde[-1], \"tanh\", derivative=True, activated_value=self.h_tilde[-1])\n",
    "        dhtilde_x = np.tile(dhtilde_x, (1, self.input_size)) # from Nx1 to NxM\n",
    "        dhtilde_x = np.sum(dhtilde_x, axis=0, keepdims=True).T # from NxM to Mx1\n",
    "        Wh_reshaped = np.sum(self.Wh, axis=0, keepdims=True).T # from NxM to Mx1\n",
    "        prev_h = np.empty((self.output_size, 1)) if len(self.h) < 2 else self.h[-2]\n",
    "        Uhprev_h = np.sum(np.tile(self.Uh @ prev_h, (1, self.input_size)), axis=0, keepdims=True).T\n",
    "        dhtilde_x *= (Wh_reshaped + dr_x * Uhprev_h)\n",
    "        \n",
    "        h_subtract = np.tile(prev_h - self.h_tilde[-1], (1, self.input_size))\n",
    "        h_subtract = np.sum(h_subtract, axis=0, keepdims=True).T\n",
    "        one_minus = np.tile((1 - self.z[-1]), (1, self.input_size))\n",
    "        one_minus = np.sum(one_minus, axis=0, keepdims=True).T\n",
    "        dh_x = dz_x * h_subtract + dhtilde_x * one_minus\n",
    "        \n",
    "        step = 1\n",
    "        \n",
    "        while step + first_index < len(self.h): \n",
    "            \n",
    "            inputs_reshaped = np.sum(self.inputs[first_index+step], axis=0, keepdims=True)\n",
    "            inputs_reshaped = np.tile(inputs_reshaped, (self.output_size, 1)) # X goes from Mx1 to Nx1\n",
    "            #print(\"first_index: \" + str(first_index))\n",
    "        \n",
    "            \"\"\"Wr gradient steps\"\"\"\n",
    "            dr_Wr = activation_function(self.r[first_index+step], \"sigmoid\", derivative=True, activated_value=self.r[first_index+step])\n",
    "            \n",
    "            dr_Wr *= (inputs_reshaped + self.Ur @ dh_Wr)\n",
    "            \n",
    "            dz_Wr = activation_function(self.z[first_index+step], \"sigmoid\", derivative=True, activated_value=self.z[first_index+step])\n",
    "\n",
    "            \n",
    "            dz_Wr *= (inputs_reshaped + self.Uz @ dh_Wr)\n",
    "            \n",
    "            dhtilde_Wr = activation_function(self.h_tilde[first_index+step], \"tanh\", derivative=True, activated_value=self.h_tilde[first_index+step])\n",
    "            \n",
    "            dhtilde_Wr *= (inputs_reshaped + dr_Wr * (self.Uh @ self.h[first_index+step-1]) + dh_Wr * (self.Uh @ self.r[first_index+step]))\n",
    "            \n",
    "            dh_Wr = dz_Wr * (self.h[first_index+step-1] - self.h_tilde[first_index+step]) \\\n",
    "            + dh_Wr * self.z[first_index+step] + dhtilde_Wr * (1 - self.z[first_index+step])\n",
    "            \n",
    "            \n",
    "            \"\"\"Wz gradient steps\"\"\"\n",
    "            \n",
    "            dr_Wz = activation_function(self.r[first_index+step], \"sigmoid\", derivative=True, activated_value=self.r[first_index+step])\n",
    "            dr_Wz *= (inputs_reshaped + self.Ur @ dh_Wz)\n",
    "            \n",
    "            dz_Wz = activation_function(self.z[first_index+step], \"sigmoid\", derivative=True, activated_value=self.z[first_index+step])\n",
    "            dz_Wz *= (inputs_reshaped + self.Uz @ dh_Wz)\n",
    "            dhtilde_Wz = activation_function(self.h_tilde[first_index+step], \"tanh\", derivative=True, activated_value=self.h_tilde[first_index+step])\n",
    "            dhtilde_Wz *= (inputs_reshaped + dr_Wz * (self.Uh @ self.h[first_index+step-1]) + dh_Wz * (self.Uh @ self.r[first_index+step]))\n",
    "            \n",
    "            dh_Wz = dz_Wz * (self.h[first_index+step-1] - self.h_tilde[first_index+step]) \\\n",
    "            + dh_Wz * self.z[first_index+step] + dhtilde_Wz * (1 - self.z[first_index+step])\n",
    "            \n",
    "            \"\"\"Uz gradient steps\"\"\"\n",
    "            \n",
    "            dz_Uz = activation_function(self.z[first_index+step], \"sigmoid\", derivative=True, activated_value=self.z[first_index+step])\n",
    "            dz_Uz *= (self.h[first_index-1+step] + self.Uz @ dh_Uz)\n",
    "            \n",
    "            dr_Uz = activation_function(self.r[first_index+step], \"sigmoid\", derivative=True, activated_value=self.r[first_index+step])\n",
    "            dr_Uz *= (self.h[first_index-1+step] + self.Ur @ dh_Uz)\n",
    "            \n",
    "            dhtilde_Uz = activation_function(self.h_tilde[first_index+step], \"tanh\", derivative=True, activated_value=self.h_tilde[first_index+step])\n",
    "            dhtilde_Uz *= ((self.Uh @ self.h[first_index-1+step]) * dr_Uz + self.r[first_index+step] * self.h[first_index-1+step] + dh_Uz * (self.Uh @ self.r[first_index+step]))\n",
    "            \n",
    "            dh_Uz = dz_Uz * (self.h[first_index-1+step]-self.h_tilde[first_index+step]) + dh_Uz * self.z[first_index+step] + dhtilde_Uz * (1 - self.z[first_index+step])\n",
    "            \n",
    "            \n",
    "            step+=1\n",
    "                \n",
    "        \n",
    "        print(str(dh_Uz.shape))\n",
    "        dh_Wr *= gradient\n",
    "        dh_Wz *= gradient\n",
    "        dh_Uz *= gradient\n",
    "        \n",
    "        dh_Wr = np.tile(dh_Wr, (1, self.input_size))\n",
    "        dh_Wz = np.tile(dh_Wz, (1, self.input_size))\n",
    "        dh_Uz = np.tile(dh_Uz, (1, self.input_size))\n",
    "        \n",
    "        self.Wr -= learning_rate * dh_Wr\n",
    "        self.Wz -= learning_rate * dh_Wz\n",
    "        self.Uz -= learning_rate * dh_Uz\n",
    "        \n",
    "        return dh_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = RNN(0.005)\n",
    "network.add_layer(3, 1)\n",
    "network.add_layer(1, 3,\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input_data = load_data_txt(\"FinalText.txt\", 5, 150)\n",
    "\n",
    "\n",
    "input_data = [\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "    [[0],[1],[0]],\n",
    "    [[1],[0],[0]],\n",
    "    [[0],[0],[1]],\n",
    "    [[0],[1],[0]],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b613a8117a12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagation_through_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m131\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-faff470bdcf4>\u001b[0m in \u001b[0;36mbackpropagation_through_time\u001b[1;34m(self, input_data, target, max_steps)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-faff470bdcf4>\u001b[0m in \u001b[0;36mdescent\u001b[1;34m(self, gradient, input_data, learning_rate, max_steps)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWr\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdh_Wr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWz\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdh_Wz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUz\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdh_Uz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdh_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,3)"
     ]
    }
   ],
   "source": [
    "epochs = 14000\n",
    "batch = 2\n",
    "early_stop = 500\n",
    "loss = 0\n",
    "prevLoss = 999\n",
    "losses = []\n",
    "\n",
    "for e in range(0, epochs+1):\n",
    "    network.clear_memory()\n",
    "    loss = 0\n",
    "    outputs = []\n",
    "    print(\"---------------\")\n",
    "    print(\"epoch \" + str(e))\n",
    "    network.learning_rate /= 1.00025\n",
    "    for i in range(0, len(input_data)-1):\n",
    "        output = network.forward_pass(input_data[i])\n",
    "        outputs.append(output)\n",
    "        loss += network.backpropagation_through_time(input_data, input_data[i+1], 8)\n",
    "  \n",
    "        if e%10 == 0 and i%131 == 0:\n",
    "            print(\"---------------\")\n",
    "            print(\"epoch \" + str(e))\n",
    "            print(\"learning rate: \" + str(network.learning_rate))\n",
    "            print(\"sample \" + str(i))\n",
    "            print(\"output: \\n\" + str(output))\n",
    "            print(\"target: \\n\" + str(input_data[i+1]))\n",
    "    \n",
    "    if e%10 == 0:        \n",
    "        print(\"-----------------average loss: \" + str(loss/len(input_data))) \n",
    "        plt.plot(np.squeeze(outputs))\n",
    "        plt.ylabel('outputs')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(np.squeeze(losses))\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        network.predict(input_data)\n",
    "        \n",
    "        \n",
    "    prevLoss = loss\n",
    "    if e > 10:\n",
    "        losses.append(loss/len(input_data))\n",
    "    if e > 90:\n",
    "        del losses[0]\n",
    "    \n",
    "    \n",
    "print(\"------------training finished successfully!------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(network.layers[-1].weights_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(str(network.layers[-2].weights_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(str(network.layers[-2].H))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
