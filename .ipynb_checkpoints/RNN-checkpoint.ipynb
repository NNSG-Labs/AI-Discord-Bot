{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images\\mainRNN.png)\n",
    "![image](images\\dWh.png)\n",
    "![image](images\\dWx.jpg)\n",
    "![image](images\\dB.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images\\dcdw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "----------------\n",
      "Input_data:\n",
      "[[0.0], [0.0]]\n",
      "Forward pass:\n",
      "[[0.0237183 ]\n",
      " [0.28809817]]\n",
      "Target Data:\n",
      "[[0.2], [0.2]]\n",
      "der_A_tiledX shape(2, 1)\n",
      "wx shape(2, 1)\n",
      "der_A_tiledX shape(2, 2)\n",
      "wx shape(2, 2)\n",
      "0 1\n",
      "----------------\n",
      "Input_data:\n",
      "[[0.2], [0.2]]\n",
      "Forward pass:\n",
      "[[-0.00537289]\n",
      " [ 0.1678916 ]]\n",
      "Target Data:\n",
      "[[0.4], [0.4]]\n",
      "der_A_tiledX shape(2, 1)\n",
      "wx shape(2, 1)\n",
      "der_A_tiledX shape(2, 2)\n",
      "wx shape(2, 2)\n",
      "0 2\n",
      "----------------\n",
      "Input_data:\n",
      "[[0.4], [0.4]]\n",
      "Forward pass:\n",
      "[[0.0322534 ]\n",
      " [0.22192716]]\n",
      "Target Data:\n",
      "[[0.6], [0.6]]\n",
      "der_A_tiledX shape(2, 1)\n",
      "wx shape(2, 2)\n",
      "der_A_tiledX shape(2, 2)\n",
      "wx shape(2, 2)\n",
      "0 3\n",
      "----------------\n",
      "Input_data:\n",
      "[[0.6], [0.6]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,2) and (1,1) not aligned: 2 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-8056ef2c23a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input_data:\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Forward pass:\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target Data:\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagation_through_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-8056ef2c23a3>\u001b[0m in \u001b[0;36mfull_forward_propagation\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;31m#print(str(self.layers[i-1].Y.shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m#From layer i-1 to layer i\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-8056ef2c23a3>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m#print(\"H shape: \" + str(self.H.shape) + \", toAppend shape: \" + str(np.array([np.dot(self.wx,input_data) + np.dot(self.wh,self.A[-1]) + self.bias]).shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mtoAppend1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoAppend1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mtoAppend2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,2) and (1,1) not aligned: 2 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "#from matplotlib import pyplot as plt\n",
    "import random, math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def swish(x):\n",
    "    return x*sigmoid(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def softmax(x):\n",
    "    p = np.exp(x - np.max(x))\n",
    "    return p/np.sum(p)\n",
    "\n",
    "def activation_function(z,act):\n",
    "    if act==\"sigmoid\":\n",
    "        return sigmoid(z)\n",
    "    elif act == \"swish\":\n",
    "        return swish(z)\n",
    "    elif act == \"relu\":\n",
    "        return relu(z)\n",
    "    elif act==\"tanh\":\n",
    "        return np.tanh(z)\n",
    "    elif act==\"softmax\":\n",
    "        return softmax(z)\n",
    "    \n",
    "class Layer():\n",
    "    def __init__(self,input_size,output_size,act=None):\n",
    "        self.size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = act\n",
    "        self.wx = np.random.uniform(-1,1,(self.output_size,self.size))\n",
    "        self.wh = np.random.uniform(-1,1,(self.output_size,self.output_size))\n",
    "        self.wy = np.random.uniform(-1,1,(self.output_size,self.output_size))\n",
    "        self.bias = np.random.uniform(-1,1,(output_size,1))\n",
    "        # Changes required to the below two definitions\n",
    "        self.A = np.random.uniform(-1,1,(0,output_size,1))\n",
    "        self.Y = np.random.uniform(-1,1,(0,output_size,1))\n",
    "        self.H = np.random.uniform(-1,1,(0,output_size,1))\n",
    "        \n",
    "    def forward_propagation(self, input_data):\n",
    "        if len(self.A)-1 >= 0:\n",
    "            #print(\"H shape: \" + str(self.H.shape) + \", toAppend shape: \" + str(np.array([np.dot(self.wx,input_data) + np.dot(self.wh,self.A[-1]) + self.bias]).shape))\n",
    "               \n",
    "            toAppend1 = np.array([np.dot(self.wx,input_data) + np.dot(self.wh,self.A[-1]) + self.bias])\n",
    "            self.H = np.append(self.H, toAppend1, axis=0)\n",
    "            toAppend2 = np.array([activation_function(self.H[-1], self.activation)])\n",
    "            self.A = np.append(self.A, toAppend2, axis=0)\n",
    "            toAppend3 = np.array([np.dot(self.wy, self.A[-1])])\n",
    "            self.Y = np.append(self.Y, toAppend3, axis=0)\n",
    "            #print(\"wy shape\" + str(self.H.shape))\n",
    "        else:\n",
    "            #print(\"wx*inppdata\"+str(np.array(input_data).shape))\n",
    "            #print(\"H\"+str(self.H.shape))\n",
    "            toAppend = np.array([np.dot(self.wx, input_data) + self.bias])\n",
    "            self.H = np.append(self.H, toAppend, axis=0)\n",
    "            #print(\"self.H: \" + str(self.H.shape))\n",
    "            self.A = activation_function(self.H, self.activation)\n",
    "            self.Y = np.array([np.dot(self.wy, self.A[-1])])\n",
    "            #print(\"Y shape\" + str(self.Y.shape))\n",
    "        #print(\"A[-1] \" + str(self.A.shape))\n",
    "    def derMSE(self, target):\n",
    "        return 2*(self.Y[-1] - target)\n",
    "    \n",
    "    def BPTT(self, input_data, gradient, learningRate):\n",
    "\n",
    "        grad = np.tile(gradient, (1,self.wy.shape[1]))\n",
    "        der_Wy_tiled = np.tile(self.A[-1].T, (self.wy.shape[0], 1))\n",
    "        #print(str(grad.shape))\n",
    "        #print(str(der_Wy_tiled.shape))\n",
    "        der_Wy = np.multiply(grad, der_Wy_tiled)\n",
    "        \n",
    "        der_A = np.multiply(grad, self.wy)\n",
    "        der_A_summed = np.sum(der_A, axis=0, keepdims=True).T\n",
    "        der_A_act = np.array(der_A_summed)\n",
    "        der_A_act.fill(1)\n",
    "        if(self.activation == \"tanh\"):\n",
    "            der_A_act = np.multiply(der_A_act, 1-np.power(self.A[-1], 2))\n",
    "        der_A_tiled = np.tile(der_A_act, (1, self.wx.shape[1]))\n",
    "        der_X = np.multiply(der_A_tiled, self.wx)\n",
    "        der_X_summed = np.sum(der_X, axis=0, keepdims=True)\n",
    "        #print(\"self.Y: \" + str(self.Y[-1]))\n",
    "        #print(\"der.X: \" + str(der_X))\n",
    "        \n",
    "        der_Wh = 0\n",
    "        der_Wx = 0\n",
    "        der_bias = 0\n",
    "        \n",
    "        der_Wh = np.empty(self.A[-1].shape)\n",
    "        der_Wh.fill(1)\n",
    "        if len(self.A) > 1:\n",
    "            for i in range(1, len(self.A)-1):\n",
    "                der_A = 0\n",
    "                if(self.activation == \"tanh\"):\n",
    "                    der_A = np.multiply(der_A, 1-np.power(self.A[i], 2))\n",
    "                der_Wh = np.multiply(der_Wh, der_A)\n",
    "                if i == len(self.A)-1:\n",
    "                    break\n",
    "                sum_Wh = np.sum(self.wh, axis=0, keepdims=True).T\n",
    "                der_Wh = np.multiply(sum_Wh, der_Wh)\n",
    "                der_Wh = der_Wh + self.A[i]\n",
    "        der_Wh = np.multiply(der_Wh, der_A_summed)\n",
    "        \n",
    "        \n",
    "        der_Wx = np.tile(np.array(input_data[0]).T, (self.wx.shape[0], 1))\n",
    "        if len(self.A) > 1:\n",
    "            for i in range(1, len(self.A)-1):\n",
    "                der_A = 0\n",
    "                if(self.activation == \"tanh\"):\n",
    "                    der_A = np.multiply(der_A, 1-np.power(self.A[i], 2))\n",
    "                der_A = np.tile(der_A, (1, self.wh.shape[1]))\n",
    "                der_Wx = np.multiply(der_Wx, der_A)\n",
    "                der_Wx = np.multiply(der_Wx, self.wh)\n",
    "                if i == len(self.A)-1:\n",
    "                    break\n",
    "                sum_Wx = np.tile(np.array(input_data[i+1]).T, (self.wx.shape[0], 1))\n",
    "                der_Wx = der_Wx + sum_Wx\n",
    "        der_A_tiledX = np.tile(der_A_summed, (1, self.wx.shape[1]))\n",
    "        print(\"der_A_tiledX shape\" + str(der_A_tiledX.shape))\n",
    "        der_Wx = np.multiply(der_Wx, der_A_tiledX)\n",
    "        \n",
    "        \"\"\"PARAMETER UPDATES\"\"\"\n",
    "        \n",
    "        self.wh = self.wh - learningRate * der_Wh\n",
    "        \n",
    "        self.wx = self.wx - learningRate * der_Wx\n",
    "        print(\"wx shape\" + str(self.wx.shape))\n",
    "        #self.bias = self.bias - learningRate * der_Wb\n",
    "        self.wy = self.wy - learningRate * der_Wy\n",
    "    \n",
    "        return der_X\n",
    "    \n",
    "    def clear(self):\n",
    "        self.A = np.random.uniform(-1,1,(0,self.output_size,1))\n",
    "        self.H = np.random.uniform(-1,1,(0,self.output_size,1))\n",
    "        self.Y = np.random.uniform(-1,1,(0,self.output_size,1))\n",
    "    \n",
    "    '''def descent(self, input_data, gradient, learningRate):\n",
    "        \n",
    "        if(self.activation == \"tanh\"):\n",
    "            derZ = 1 - np.power(self.A, 2)\n",
    "        elif(self.activation == \"swish\"):\n",
    "            derZ = swish(self.Z) + sigmoid(self.Z) * (1 - swish(self.Z))\n",
    "        reps = (self.weights.shape[0], 1)\n",
    "        derWeights = np.tile(input_data.transpose(), reps)        # Size -> a(L-1)*a(L)\n",
    "        \"\"\"derWeights is a matrix with derivatives of Z WRT weights, which is transposed inputs, \n",
    "        repeated in rows n-times, where n is number of neurons.\n",
    "        \n",
    "    Example:\n",
    "    \n",
    "        input_data = [\n",
    "            [2],\n",
    "            [1],\n",
    "            [0]\n",
    "        ]\n",
    "        \n",
    "        derWeights = [\n",
    "            [2, 1, 0],\n",
    "            [2, 1, 0],\n",
    "            [2, 1, 0],\n",
    "            ... n-rows\n",
    "        ]\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        \"\"\" The below part adds the gradient to the derivative of A WRT input_data and passes this new\n",
    "        gradient through return, to be used as the gradient for next layer's descent.\n",
    "            dA/dX is made with 2 steps: adding the backprop gradient to the derivative of A WRT Z and then adding\n",
    "        the derivative of Z WRT input_data (chain rule).\n",
    "        \n",
    "        1.\n",
    "        Since A is a matrix shaped Nx1, where N is the number of outputs, the receiving gradient from the upper layer\n",
    "        must be the same shape. Therefore we can multiply the gradient and the derivative together element-wise.\n",
    "        \n",
    "    Example:\n",
    "        \n",
    "         derZ = [3x1]\n",
    "         gradient = [3x1]\n",
    "         firstGrad = [3x1] *(elementwise) [3x1] = [3x1]\n",
    "        \n",
    "        \"\"\"\n",
    "        firstGrad = np.multiply(gradient, derZ)             # ∂C/∂a(L) * ∂(act)/∂Z    a(L)*1 * a(L)*1 = a(L)*1\n",
    "        \n",
    "        \"\"\"\n",
    "        2.\n",
    "        What's left is adding the gradient of Z WRT input_data. \n",
    "        \n",
    "        This turns out to be the weights matrix. Now we have to multiply the firstGrad gradient to these weights \n",
    "        element-wise but since the firstGrad is Nx1 shape and the weights are NxM, where M are the features, \n",
    "        we need to reshape the gradient matrix to match the weights matrix by cloning gradient's columns:\n",
    "        \"\"\"\n",
    "        secondGrad = np.tile(firstGrad, (1, self.weights.shape[1]))          #Size -> a(L)*1 -> a(L)*a(L-1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Finally we multiply (E-W) secondGrad to the weights matrix:\n",
    "        \"\"\"\n",
    "        derX = np.multiply(self.weights,secondGrad)         #Size -> a(L)*a(L-1) * a(L)*a(L-1) = a(L)*a(L-1)\n",
    "        \n",
    "        \"\"\"But because same inputs are multiplied with many weights, we can sum those weights together. It turns out\n",
    "        that we can sum columns to do that\"\"\"\n",
    "        \n",
    "        derX = np.sum(derX, axis=0, keepdims=True)       #Size -> 1*a(L-1) \n",
    "        derBias = 1\n",
    "        \n",
    "        weightGrad = np.multiply(derWeights, np.tile(firstGrad, (1, self.weights.shape[1])))\n",
    "        self.weights = self.weights - learningRate * weightGrad\n",
    "        self.bias = self.bias - learningRate * firstGrad\n",
    "        \n",
    "        \"\"\"We return transposed matrix, because we desire inputs with a shape of Nx1 and right now finalGrad is \n",
    "        transposed\"\"\"\n",
    "        return derX.transpose()     # Size -> a(L-1)*1\n",
    "    \n",
    "        \n",
    "\"\"\"class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "        self.epochs=10\n",
    "        self.learning_rate = 0.008\n",
    "    \n",
    "    def add_layer(self,input_size,output_size,activation=None):\n",
    "        new_layer = Layer(input_size,output_size,activation)\n",
    "        self.layers.append(new_layer)\n",
    "        \n",
    "    def forward_propagation(self,layer_no):\n",
    "        current_layer = self.layers[layer_no-1]\n",
    "        prev_layer = self.layers[layer_no-2]\n",
    "        act = current_layer.activation\n",
    "        input_data = prev_layer.A\n",
    "        self.Z = np.dot(weights,input_data)+self.bias\n",
    "        result = activation_function(self.Z,act)    # array containing neuron values\n",
    "        current_layer.A = result            #After forward propogation, fills in the neurons in that layer\n",
    "        return result\n",
    "    \n",
    "    def full_forward_propagation(self, input_data):\n",
    "        #print(\"layer 0 forward_propagation\")\n",
    "        \n",
    "        self.layers[0].forward_propagation(input_data)        # From input data to first layer\n",
    "        for i in range(1, len(self.layers)):\n",
    "            #print(\"layer \" + str(i) + \" forward_propagation\")\n",
    "            \n",
    "            self.layers[i].forward_propagation(self.layers[i-1].A)      #From layer i-1 to layer i \n",
    "        return self.layers[len(self.layers)-1].A\n",
    "    \n",
    "    def back_propagation(self, input_data, target):\n",
    "        gradient = self.layers[len(self.layers)-1].derMSE(target)     # ∂C/∂a(L)   Size -> a(L)*1\n",
    "        for i in range(0, len(self.layers)-1):\n",
    "            index = len(self.layers)-1 - i\n",
    "            #print(\"Layer \" + str(index) + \" backpropagation\")\n",
    "            gradient = self.layers[index].descent(self.layers[index-1].A, gradient, self.learning_rate)   #a(L)*1\n",
    "        self.layers[0].descent(input_data, gradient, self.learning_rate)\n",
    "            \n",
    "            \n",
    "    def predict(self,test_data):\n",
    "        self.layers[0].forward_propagation(test_data)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].forward_propagation(self.layers[i-1].A)\n",
    "        return self.layers[len(self.layers)-1].A\"\"\"\n",
    "        '''\n",
    "train_data = [\n",
    "    [[0.0],[0.0]],\n",
    "    [[0.2],[0.2]],\n",
    "    [[0.4],[0.4]],\n",
    "    [[0.6],[0.6]],\n",
    "    [[0.8],[0.8]]\n",
    "]\n",
    "\n",
    "class RNN():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.008\n",
    "    \n",
    "    def add_layer(self,input_size,output_size,activation=None):\n",
    "        new_layer = Layer(input_size,output_size,activation)\n",
    "        self.layers.append(new_layer)\n",
    "        \n",
    "    def forward_propagation(self,layer_no):\n",
    "        current_layer = self.layers[layer_no-1]\n",
    "        prev_layer = self.layers[layer_no-2]\n",
    "        act = current_layer.activation\n",
    "        input_data = prev_layer.A\n",
    "        self.H = np.dot(weights,input_data)+self.bias\n",
    "        result = activation_function(self.H,act)    # array containing neuron values\n",
    "        current_layer.A = result            #After forward propogation, fills in the neurons in that layer\n",
    "        return result\n",
    "    \n",
    "    def full_forward_propagation(self, input_data):\n",
    "        #print(\"layer 0 forward_propagation\")\n",
    "        #print(\"Input_data shape \" + str(np.array(input_data).shape))\n",
    "        self.layers[0].forward_propagation(input_data)        # From input data to first layer\n",
    "        for i in range(1, len(self.layers)):\n",
    "            #print(\"layer \" + str(i) + \" forward_propagation\")\n",
    "            \n",
    "            #print(str(self.layers[i-1].Y.shape))\n",
    "            self.layers[i].forward_propagation(self.layers[i-1].Y[-1])      #From layer i-1 to layer i \n",
    "        return self.layers[-1].Y[-1]\n",
    "    \n",
    "    def back_propagation_through_time(self,input_data,target):\n",
    "        grad = self.layers[-1].derMSE(target)\n",
    "        #print(\"First MSE grad\" + str(grad))\n",
    "        for i in range(1, len(self.layers)-1):\n",
    "            #print(\"inside loop, iteration \" + str(i))\n",
    "            grad = self.layers[-i].BPTT(self.layers[-i-1].Y, grad, self.learning_rate)\n",
    "        self.layers[0].BPTT(input_data, grad, self.learning_rate)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        for i in range(0, len(self.layers)):\n",
    "            self.layers[i].clear()\n",
    "        \n",
    "network = RNN()\n",
    "network.add_layer(2, 2, \"tanh\")\n",
    "network.add_layer(2, 1, \"tanh\")\n",
    "network.add_layer(1, 2, \"tanh\")\n",
    "\n",
    "for i in range(0, 16000):\n",
    "    network.clear_memory()\n",
    "    for j in range(0,len(train_data)-1):\n",
    "        if i%399==0 and j%1 == 0:\n",
    "            print(i,j)\n",
    "            print(\"----------------\")\n",
    "            print(\"Input_data:\\n\" + str(train_data[j]))\n",
    "            print(\"Forward pass:\\n\" + str(network.full_forward_propagation(train_data[j])))\n",
    "            print(\"Target Data:\\n\" + str(train_data[j+1]))\n",
    "            network.back_propagation_through_time(train_data, train_data[j+1])\n",
    "        else:\n",
    "            network.full_forward_propagation(train_data[j])\n",
    "            network.back_propagation_through_time(train_data, train_data[j+1])\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Model Training Completed Successfully !!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.empty((0, 3, 1))\n",
    "while len(test_data) < 20:\n",
    "    X_loc = random.random()*2-1\n",
    "    Y_loc = random.random()*2-1\n",
    "    radius = random.random()\n",
    "    test_data = np.append(test_data, [[[X_loc], [Y_loc], [radius]]], axis=0)\n",
    "for x in test_data:\n",
    "    pass\n",
    "    #print(\"Prediction :\\n {} \\n ----------------\\n\".format(x), network.predict(x),\"\\n--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
