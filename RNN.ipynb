{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](dcdw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-949fda6a32f9>, line 173)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-949fda6a32f9>\"\u001b[0;36m, line \u001b[0;32m173\u001b[0m\n\u001b[0;31m    network = NeuralNetwork()\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random, math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def swish(x):\n",
    "    return x*sigmoid(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def softmax(x):\n",
    "    p = np.exp(x - np.max(x))\n",
    "    return p/np.sum(p)\n",
    "\n",
    "def activation_function(z,act):\n",
    "    if act==\"sigmoid\":\n",
    "        return sigmoid(z)\n",
    "    elif act == \"swish\":\n",
    "        return swish(z)\n",
    "    elif act == \"relu\":\n",
    "        return relu(z)\n",
    "    elif act==\"tanh\":\n",
    "        return np.tanh(z)\n",
    "    elif act==\"softmax\":\n",
    "        return softmax(z)\n",
    "    \n",
    "class Layer():\n",
    "    def __init__(self,input_size,output_size,act=None):\n",
    "        self.size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = act\n",
    "        self.wx = np.random.uniform(-1,1,(self.output_size,self.size))\n",
    "        self.wh = np.random.uniform(-1,1,(self.size,self.size))\n",
    "        self.wy = np.random.uniform(-1,1,(self.size,self.size))\n",
    "        self.bias = np.random.uniform(-1,1,(output_size,1))\n",
    "        # Changes required to the below two definitions\n",
    "        self.A = np.random.uniform(-1,1,(output_size,1,))\n",
    "        self.Z = np.random.uniform(-1,1,(output_size,1))\n",
    "        \n",
    "    def forward_propagation(self, input_data):\n",
    "        self.Z = np.dot(self.weights,input_data)+self.bias\n",
    "        self.A = activation_function(self.Z, self.activation)\n",
    "        \n",
    "    def derMSE(self, target):\n",
    "        return 2*(self.A - target)\n",
    "    \n",
    "    '''def descent(self, input_data, gradient, learningRate):\n",
    "        \n",
    "        if(self.activation == \"tanh\"):\n",
    "            derZ = 1 - np.power(self.A, 2)\n",
    "        elif(self.activation == \"swish\"):\n",
    "            derZ = swish(self.Z) + sigmoid(self.Z) * (1 - swish(self.Z))\n",
    "        reps = (self.weights.shape[0], 1)\n",
    "        derWeights = np.tile(input_data.transpose(), reps)        # Size -> a(L-1)*a(L)\n",
    "        \"\"\"derWeights is a matrix with derivatives of Z WRT weights, which is transposed inputs, \n",
    "        repeated in rows n-times, where n is number of neurons.\n",
    "        \n",
    "    Example:\n",
    "    \n",
    "        input_data = [\n",
    "            [2],\n",
    "            [1],\n",
    "            [0]\n",
    "        ]\n",
    "        \n",
    "        derWeights = [\n",
    "            [2, 1, 0],\n",
    "            [2, 1, 0],\n",
    "            [2, 1, 0],\n",
    "            ... n-rows\n",
    "        ]\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        \"\"\" The below part adds the gradient to the derivative of A WRT input_data and passes this new\n",
    "        gradient through return, to be used as the gradient for next layer's descent.\n",
    "            dA/dX is made with 2 steps: adding the backprop gradient to the derivative of A WRT Z and then adding\n",
    "        the derivative of Z WRT input_data (chain rule).\n",
    "        \n",
    "        1.\n",
    "        Since A is a matrix shaped Nx1, where N is the number of outputs, the receiving gradient from the upper layer\n",
    "        must be the same shape. Therefore we can multiply the gradient and the derivative together element-wise.\n",
    "        \n",
    "    Example:\n",
    "        \n",
    "         derZ = [3x1]\n",
    "         gradient = [3x1]\n",
    "         firstGrad = [3x1] *(elementwise) [3x1] = [3x1]\n",
    "        \n",
    "        \"\"\"\n",
    "        firstGrad = np.multiply(gradient, derZ)             # ∂C/∂a(L) * ∂(act)/∂Z    a(L)*1 * a(L)*1 = a(L)*1\n",
    "        \n",
    "        \"\"\"\n",
    "        2.\n",
    "        What's left is adding the gradient of Z WRT input_data. \n",
    "        \n",
    "        This turns out to be the weights matrix. Now we have to multiply the firstGrad gradient to these weights \n",
    "        element-wise but since the firstGrad is Nx1 shape and the weights are NxM, where M are the features, \n",
    "        we need to reshape the gradient matrix to match the weights matrix by cloning gradient's columns:\n",
    "        \"\"\"\n",
    "        secondGrad = np.tile(firstGrad, (1, self.weights.shape[1]))          #Size -> a(L)*1 -> a(L)*a(L-1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Finally we multiply (E-W) secondGrad to the weights matrix:\n",
    "        \"\"\"\n",
    "        derX = np.multiply(self.weights,secondGrad)         #Size -> a(L)*a(L-1) * a(L)*a(L-1) = a(L)*a(L-1)\n",
    "        \n",
    "        \"\"\"But because same inputs are multiplied with many weights, we can sum those weights together. It turns out\n",
    "        that we can sum columns to do that\"\"\"\n",
    "        \n",
    "        derX = np.sum(derX, axis=0, keepdims=True)       #Size -> 1*a(L-1) \n",
    "        derBias = 1\n",
    "        \n",
    "        weightGrad = np.multiply(derWeights, np.tile(firstGrad, (1, self.weights.shape[1])))\n",
    "        self.weights = self.weights - learningRate * weightGrad\n",
    "        self.bias = self.bias - learningRate * firstGrad\n",
    "        \n",
    "        \"\"\"We return transposed matrix, because we desire inputs with a shape of Nx1 and right now finalGrad is \n",
    "        transposed\"\"\"\n",
    "        return derX.transpose()     # Size -> a(L-1)*1\n",
    "    \n",
    "        \n",
    "\"\"\"class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "        self.epochs=10\n",
    "        self.learning_rate = 0.008\n",
    "    \n",
    "    def add_layer(self,input_size,output_size,activation=None):\n",
    "        new_layer = Layer(input_size,output_size,activation)\n",
    "        self.layers.append(new_layer)\n",
    "        \n",
    "    def forward_propagation(self,layer_no):\n",
    "        current_layer = self.layers[layer_no-1]\n",
    "        prev_layer = self.layers[layer_no-2]\n",
    "        act = current_layer.activation\n",
    "        input_data = prev_layer.A\n",
    "        self.Z = np.dot(weights,input_data)+self.bias\n",
    "        result = activation_function(self.Z,act)    # array containing neuron values\n",
    "        current_layer.A = result            #After forward propogation, fills in the neurons in that layer\n",
    "        return result\n",
    "    \n",
    "    def full_forward_propagation(self, input_data):\n",
    "        #print(\"layer 0 forward_propagation\")\n",
    "        \n",
    "        self.layers[0].forward_propagation(input_data)        # From input data to first layer\n",
    "        for i in range(1, len(self.layers)):\n",
    "            #print(\"layer \" + str(i) + \" forward_propagation\")\n",
    "            \n",
    "            self.layers[i].forward_propagation(self.layers[i-1].A)      #From layer i-1 to layer i \n",
    "        return self.layers[len(self.layers)-1].A\n",
    "    \n",
    "    def back_propagation(self, input_data, target):\n",
    "        gradient = self.layers[len(self.layers)-1].derMSE(target)     # ∂C/∂a(L)   Size -> a(L)*1\n",
    "        for i in range(0, len(self.layers)-1):\n",
    "            index = len(self.layers)-1 - i\n",
    "            #print(\"Layer \" + str(index) + \" backpropagation\")\n",
    "            gradient = self.layers[index].descent(self.layers[index-1].A, gradient, self.learning_rate)   #a(L)*1\n",
    "        self.layers[0].descent(input_data, gradient, self.learning_rate)\n",
    "            \n",
    "            \n",
    "    def predict(self,test_data):\n",
    "        self.layers[0].forward_propagation(test_data)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].forward_propagation(self.layers[i-1].A)\n",
    "        return self.layers[len(self.layers)-1].A\"\"\"\n",
    "        '''\n",
    "\n",
    "\n",
    "class RNN():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.008\n",
    "    \n",
    "    def add_layer(self,input_size,output_size,activation=None):\n",
    "        new_layer = Layer(input_size,output_size,activation)\n",
    "        self.layers.append(new_layer)\n",
    "        \n",
    "    def forward_propagation(self,layer_no):\n",
    "        current_layer = self.layers[layer_no-1]\n",
    "        prev_layer = self.layers[layer_no-2]\n",
    "        act = current_layer.activation\n",
    "        input_data = prev_layer.A\n",
    "        self.Z = np.dot(weights,input_data)+self.bias\n",
    "        result = activation_function(self.Z,act)    # array containing neuron values\n",
    "        current_layer.A = result            #After forward propogation, fills in the neurons in that layer\n",
    "        return result\n",
    "    \n",
    "    def full_forward_propagation(self, input_data):\n",
    "        #print(\"layer 0 forward_propagation\")\n",
    "        \n",
    "        self.layers[0].forward_propagation(input_data)        # From input data to first layer\n",
    "        for i in range(1, len(self.layers)):\n",
    "            #print(\"layer \" + str(i) + \" forward_propagation\")\n",
    "            \n",
    "            self.layers[i].forward_propagation(self.layers[i-1].A)      #From layer i-1 to layer i \n",
    "        return self.layers[len(self.layers)-1].A\n",
    "    \n",
    "    def back_propogation_through_time(self,output_data,target):\n",
    "        \n",
    "        \n",
    "network = NeuralNetwork()\n",
    "network.add_layer(3, 3, \"tanh\")\n",
    "network.add_layer(3, 2, \"tanh\")\n",
    "network.add_layer(2, 2, \"swish\")\n",
    "\n",
    "for i in range(0, 800):\n",
    "    for j in range(0,len(train_data)):\n",
    "        if i%95==0 and j%19 == 0:\n",
    "            print(i,j)\n",
    "            print(\"----------------\")\n",
    "            print(\"Input_data:\\n\" + str(train_data[j]))\n",
    "            print(\"Forward pass:\\n\" + str(network.full_forward_propagation(train_data[j])))\n",
    "            print(\"Target Data:\\n\" + str(target_data[j]))\n",
    "            network.back_propagation(train_data[j], target_data[j])\n",
    "        else:\n",
    "            network.full_forward_propagation(train_data[j])\n",
    "            network.back_propagation(train_data[j], target_data[j])\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Model Training Completed Successfully !!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :\n",
      " [[-0.00619986]\n",
      " [-0.37720284]\n",
      " [ 0.6463399 ]] \n",
      " ----------------\n",
      " [[0.86187742]\n",
      " [0.10832696]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.78086656]\n",
      " [-0.32564749]\n",
      " [ 0.66764041]] \n",
      " ----------------\n",
      " [[0.05019719]\n",
      " [0.99568981]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.41540763]\n",
      " [0.65871535]\n",
      " [0.78331597]] \n",
      " ----------------\n",
      " [[0.00845399]\n",
      " [1.03813781]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[ 0.24722475]\n",
      " [-0.24805785]\n",
      " [ 0.53004415]] \n",
      " ----------------\n",
      " [[0.91786636]\n",
      " [0.07840574]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.70279271]\n",
      " [ 0.46321518]\n",
      " [ 0.61447053]] \n",
      " ----------------\n",
      " [[0.02247825]\n",
      " [1.0280764 ]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.17676664]\n",
      " [ 0.2792578 ]\n",
      " [ 0.2540299 ]] \n",
      " ----------------\n",
      " [[0.92570571]\n",
      " [0.07711444]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.07945405]\n",
      " [ 0.11611852]\n",
      " [ 0.58647246]] \n",
      " ----------------\n",
      " [[0.92530286]\n",
      " [0.07696259]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[ 0.42428731]\n",
      " [-0.80201862]\n",
      " [ 0.86485241]] \n",
      " ----------------\n",
      " [[-0.00842461]\n",
      " [ 1.03266868]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.58921302]\n",
      " [0.99663906]\n",
      " [0.38109458]] \n",
      " ----------------\n",
      " [[0.00563894]\n",
      " [1.03932788]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.9959181 ]\n",
      " [0.52394166]\n",
      " [0.80544732]] \n",
      " ----------------\n",
      " [[-0.02351763]\n",
      " [ 1.00170404]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.89598111]\n",
      " [0.30033887]\n",
      " [0.06917741]] \n",
      " ----------------\n",
      " [[-0.01679617]\n",
      " [ 1.02353098]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.23299178]\n",
      " [ 0.1817426 ]\n",
      " [ 0.85213741]] \n",
      " ----------------\n",
      " [[0.55650583]\n",
      " [0.35144367]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.36102798]\n",
      " [0.13203824]\n",
      " [0.84910971]] \n",
      " ----------------\n",
      " [[0.86649165]\n",
      " [0.10585228]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.57679619]\n",
      " [ 0.94919995]\n",
      " [ 0.31126809]] \n",
      " ----------------\n",
      " [[-0.02502604]\n",
      " [ 0.9943119 ]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.29478427]\n",
      " [0.98395435]\n",
      " [0.52029562]] \n",
      " ----------------\n",
      " [[0.00129174]\n",
      " [1.04013317]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.91626869]\n",
      " [0.82537921]\n",
      " [0.40565307]] \n",
      " ----------------\n",
      " [[-0.00432107]\n",
      " [ 1.03903566]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.2812659 ]\n",
      " [0.99565242]\n",
      " [0.49594973]] \n",
      " ----------------\n",
      " [[0.00119708]\n",
      " [1.04013859]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.59998733]\n",
      " [0.76773847]\n",
      " [0.86732206]] \n",
      " ----------------\n",
      " [[-0.00184795]\n",
      " [ 1.03979696]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.9030003 ]\n",
      " [ 0.44789778]\n",
      " [ 0.27488716]] \n",
      " ----------------\n",
      " [[0.07280535]\n",
      " [0.96463864]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.37148568]\n",
      " [0.50592229]\n",
      " [0.2266818 ]] \n",
      " ----------------\n",
      " [[0.41531909]\n",
      " [0.49951002]] \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_data = np.empty((0, 3, 1))\n",
    "while len(test_data) < 20:\n",
    "    X_loc = random.random()*2-1\n",
    "    Y_loc = random.random()*2-1\n",
    "    radius = random.random()\n",
    "    test_data = np.append(test_data, [[[X_loc], [Y_loc], [radius]]], axis=0)\n",
    "for x in test_data:\n",
    "    print(\"Prediction :\\n {} \\n ----------------\\n\".format(x), network.predict(x),\"\\n--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
