{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](dcdw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "----------------\n",
      "Input_data:\n",
      "[[0], [0]]\n",
      "wx*inppdata(2, 1)\n",
      "H(0, 3, 1)\n",
      "To Append: (1, 3, 1)\n",
      "Y shape(3, 1)\n",
      "A[-1] (3, 1)\n",
      "wx*inppdata(3, 1)\n",
      "H(0, 2, 1)\n",
      "To Append: (1, 2, 1)\n",
      "Y shape(2, 1)\n",
      "A[-1] (2, 1)\n",
      "wx*inppdata(2, 1)\n",
      "H(0, 2, 1)\n",
      "To Append: (1, 2, 1)\n",
      "Y shape(2, 1)\n",
      "A[-1] (2, 1)\n",
      "Forward pass:\n",
      "[[-0.05687134]\n",
      " [-0.35245673]]\n",
      "Target Data:\n",
      "[[0], [1]]\n",
      "0 1\n",
      "----------------\n",
      "Input_data:\n",
      "[[0], [1]]\n",
      "Y shape(3, 3)\n",
      "A[-1] ()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 1 and the array at index 1 has size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-36bb0223b22d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input_data:\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Forward pass:\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target Data:\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;31m#network.back_propagation_through_time(train_data[j], train_data[j+1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-36bb0223b22d>\u001b[0m in \u001b[0;36mfull_forward_propagation\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;31m#print(\"layer \" + str(i) + \" forward_propagation\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m#From layer i-1 to layer i\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-36bb0223b22d>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   4691\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4692\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4693\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 1 and the array at index 1 has size 3"
     ]
    }
   ],
   "source": [
    "#from matplotlib import pyplot as plt\n",
    "import random, math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def swish(x):\n",
    "    return x*sigmoid(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def softmax(x):\n",
    "    p = np.exp(x - np.max(x))\n",
    "    return p/np.sum(p)\n",
    "\n",
    "def activation_function(z,act):\n",
    "    if act==\"sigmoid\":\n",
    "        return sigmoid(z)\n",
    "    elif act == \"swish\":\n",
    "        return swish(z)\n",
    "    elif act == \"relu\":\n",
    "        return relu(z)\n",
    "    elif act==\"tanh\":\n",
    "        return np.tanh(z)\n",
    "    elif act==\"softmax\":\n",
    "        return softmax(z)\n",
    "    \n",
    "class Layer():\n",
    "    def __init__(self,input_size,output_size,act=None):\n",
    "        self.size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = act\n",
    "        self.wx = np.random.uniform(-1,1,(self.output_size,self.size))\n",
    "        self.wh = np.random.uniform(-1,1,(self.output_size,self.output_size))\n",
    "        self.wy = np.random.uniform(-1,1,(self.output_size,self.output_size))\n",
    "        self.bias = np.random.uniform(-1,1,(output_size,1))\n",
    "        # Changes required to the below two definitions\n",
    "        self.A = np.random.uniform(-1,1,(0,output_size,1))\n",
    "        self.Y = np.random.uniform(-1,1,(output_size,1))\n",
    "        self.H = np.random.uniform(-1,1,(0,output_size,1))\n",
    "        \n",
    "    def forward_propagation(self, input_data):\n",
    "        if len(self.A)-1 >= 0:\n",
    "            self.H = np.append(self.H, [np.dot(self.wx,input_data) + np.dot(self.wh,self.A[-1]) + self.bias], axis=0)\n",
    "            self.A = np.append(self.A, activation_function(self.H[-1], self.activation))\n",
    "            self.Y = np.dot(self.wy, self.A[-1])\n",
    "            print(\"wy shape\" + str(self.wy.shape))\n",
    "        else:\n",
    "            print(\"wx*inppdata\"+str(np.array(input_data).shape))\n",
    "            print(\"H\"+str(self.H.shape))\n",
    "            toAppend = np.array([np.dot(self.wx, input_data) + self.bias])\n",
    "            self.H = np.append(self.H, toAppend, axis=0)\n",
    "            print(\"To Append: \" + str(toAppend.shape))\n",
    "            self.A = activation_function(self.H, self.activation)\n",
    "            self.Y = np.dot(self.wy, self.A[-1])\n",
    "            print(\"Y shape\" + str(self.Y.shape))\n",
    "        print(\"A[-1] \" + str(self.A[-1].shape))\n",
    "    def derMSE(self, target):\n",
    "        return 2*(self.A - target)\n",
    "    \n",
    "    def BPTT(self, input_data, gradient, learningRate):\n",
    "        pass\n",
    "    \n",
    "    def clear(self):\n",
    "        self.A = np.random.uniform(-1,1,(0,self.output_size,1))\n",
    "        self.H = np.random.uniform(-1,1,(0,self.output_size,1))\n",
    "        \n",
    "    \n",
    "    '''def descent(self, input_data, gradient, learningRate):\n",
    "        \n",
    "        if(self.activation == \"tanh\"):\n",
    "            derZ = 1 - np.power(self.A, 2)\n",
    "        elif(self.activation == \"swish\"):\n",
    "            derZ = swish(self.Z) + sigmoid(self.Z) * (1 - swish(self.Z))\n",
    "        reps = (self.weights.shape[0], 1)\n",
    "        derWeights = np.tile(input_data.transpose(), reps)        # Size -> a(L-1)*a(L)\n",
    "        \"\"\"derWeights is a matrix with derivatives of Z WRT weights, which is transposed inputs, \n",
    "        repeated in rows n-times, where n is number of neurons.\n",
    "        \n",
    "    Example:\n",
    "    \n",
    "        input_data = [\n",
    "            [2],\n",
    "            [1],\n",
    "            [0]\n",
    "        ]\n",
    "        \n",
    "        derWeights = [\n",
    "            [2, 1, 0],\n",
    "            [2, 1, 0],\n",
    "            [2, 1, 0],\n",
    "            ... n-rows\n",
    "        ]\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        \"\"\" The below part adds the gradient to the derivative of A WRT input_data and passes this new\n",
    "        gradient through return, to be used as the gradient for next layer's descent.\n",
    "            dA/dX is made with 2 steps: adding the backprop gradient to the derivative of A WRT Z and then adding\n",
    "        the derivative of Z WRT input_data (chain rule).\n",
    "        \n",
    "        1.\n",
    "        Since A is a matrix shaped Nx1, where N is the number of outputs, the receiving gradient from the upper layer\n",
    "        must be the same shape. Therefore we can multiply the gradient and the derivative together element-wise.\n",
    "        \n",
    "    Example:\n",
    "        \n",
    "         derZ = [3x1]\n",
    "         gradient = [3x1]\n",
    "         firstGrad = [3x1] *(elementwise) [3x1] = [3x1]\n",
    "        \n",
    "        \"\"\"\n",
    "        firstGrad = np.multiply(gradient, derZ)             # ∂C/∂a(L) * ∂(act)/∂Z    a(L)*1 * a(L)*1 = a(L)*1\n",
    "        \n",
    "        \"\"\"\n",
    "        2.\n",
    "        What's left is adding the gradient of Z WRT input_data. \n",
    "        \n",
    "        This turns out to be the weights matrix. Now we have to multiply the firstGrad gradient to these weights \n",
    "        element-wise but since the firstGrad is Nx1 shape and the weights are NxM, where M are the features, \n",
    "        we need to reshape the gradient matrix to match the weights matrix by cloning gradient's columns:\n",
    "        \"\"\"\n",
    "        secondGrad = np.tile(firstGrad, (1, self.weights.shape[1]))          #Size -> a(L)*1 -> a(L)*a(L-1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Finally we multiply (E-W) secondGrad to the weights matrix:\n",
    "        \"\"\"\n",
    "        derX = np.multiply(self.weights,secondGrad)         #Size -> a(L)*a(L-1) * a(L)*a(L-1) = a(L)*a(L-1)\n",
    "        \n",
    "        \"\"\"But because same inputs are multiplied with many weights, we can sum those weights together. It turns out\n",
    "        that we can sum columns to do that\"\"\"\n",
    "        \n",
    "        derX = np.sum(derX, axis=0, keepdims=True)       #Size -> 1*a(L-1) \n",
    "        derBias = 1\n",
    "        \n",
    "        weightGrad = np.multiply(derWeights, np.tile(firstGrad, (1, self.weights.shape[1])))\n",
    "        self.weights = self.weights - learningRate * weightGrad\n",
    "        self.bias = self.bias - learningRate * firstGrad\n",
    "        \n",
    "        \"\"\"We return transposed matrix, because we desire inputs with a shape of Nx1 and right now finalGrad is \n",
    "        transposed\"\"\"\n",
    "        return derX.transpose()     # Size -> a(L-1)*1\n",
    "    \n",
    "        \n",
    "\"\"\"class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "        self.epochs=10\n",
    "        self.learning_rate = 0.008\n",
    "    \n",
    "    def add_layer(self,input_size,output_size,activation=None):\n",
    "        new_layer = Layer(input_size,output_size,activation)\n",
    "        self.layers.append(new_layer)\n",
    "        \n",
    "    def forward_propagation(self,layer_no):\n",
    "        current_layer = self.layers[layer_no-1]\n",
    "        prev_layer = self.layers[layer_no-2]\n",
    "        act = current_layer.activation\n",
    "        input_data = prev_layer.A\n",
    "        self.Z = np.dot(weights,input_data)+self.bias\n",
    "        result = activation_function(self.Z,act)    # array containing neuron values\n",
    "        current_layer.A = result            #After forward propogation, fills in the neurons in that layer\n",
    "        return result\n",
    "    \n",
    "    def full_forward_propagation(self, input_data):\n",
    "        #print(\"layer 0 forward_propagation\")\n",
    "        \n",
    "        self.layers[0].forward_propagation(input_data)        # From input data to first layer\n",
    "        for i in range(1, len(self.layers)):\n",
    "            #print(\"layer \" + str(i) + \" forward_propagation\")\n",
    "            \n",
    "            self.layers[i].forward_propagation(self.layers[i-1].A)      #From layer i-1 to layer i \n",
    "        return self.layers[len(self.layers)-1].A\n",
    "    \n",
    "    def back_propagation(self, input_data, target):\n",
    "        gradient = self.layers[len(self.layers)-1].derMSE(target)     # ∂C/∂a(L)   Size -> a(L)*1\n",
    "        for i in range(0, len(self.layers)-1):\n",
    "            index = len(self.layers)-1 - i\n",
    "            #print(\"Layer \" + str(index) + \" backpropagation\")\n",
    "            gradient = self.layers[index].descent(self.layers[index-1].A, gradient, self.learning_rate)   #a(L)*1\n",
    "        self.layers[0].descent(input_data, gradient, self.learning_rate)\n",
    "            \n",
    "            \n",
    "    def predict(self,test_data):\n",
    "        self.layers[0].forward_propagation(test_data)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].forward_propagation(self.layers[i-1].A)\n",
    "        return self.layers[len(self.layers)-1].A\"\"\"\n",
    "        '''\n",
    "train_data = [\n",
    "    [[0],[0]],\n",
    "    [[0],[1]],\n",
    "    [[1],[0]],\n",
    "    [[1],[1]]\n",
    "]\n",
    "\n",
    "class RNN():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.008\n",
    "    \n",
    "    def add_layer(self,input_size,output_size,activation=None):\n",
    "        new_layer = Layer(input_size,output_size,activation)\n",
    "        self.layers.append(new_layer)\n",
    "        \n",
    "    def forward_propagation(self,layer_no):\n",
    "        current_layer = self.layers[layer_no-1]\n",
    "        prev_layer = self.layers[layer_no-2]\n",
    "        act = current_layer.activation\n",
    "        input_data = prev_layer.A\n",
    "        self.H = np.dot(weights,input_data)+self.bias\n",
    "        result = activation_function(self.H,act)    # array containing neuron values\n",
    "        current_layer.A = result            #After forward propogation, fills in the neurons in that layer\n",
    "        return result\n",
    "    \n",
    "    def full_forward_propagation(self, input_data):\n",
    "        #print(\"layer 0 forward_propagation\")\n",
    "        \n",
    "        self.layers[0].forward_propagation(input_data)        # From input data to first layer\n",
    "        for i in range(1, len(self.layers)):\n",
    "            #print(\"layer \" + str(i) + \" forward_propagation\")\n",
    "            \n",
    "            self.layers[i].forward_propagation(self.layers[i-1].Y)      #From layer i-1 to layer i \n",
    "        return self.layers[len(self.layers)-1].Y\n",
    "    \n",
    "    def back_propagation_through_time(self,output_data,target):\n",
    "        pass\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        for i in range(0, len(self.layers)):\n",
    "            self.layers[i].clear()\n",
    "        \n",
    "network = RNN()\n",
    "network.add_layer(2, 3, \"tanh\")\n",
    "network.add_layer(3, 2, \"tanh\")\n",
    "network.add_layer(2, 2, \"swish\")\n",
    "\n",
    "for i in range(0, 800):\n",
    "    network.clear_memory()\n",
    "    for j in range(0,len(train_data)-1):\n",
    "        if i%201==0 and j%1 == 0:\n",
    "            print(i,j)\n",
    "            print(\"----------------\")\n",
    "            print(\"Input_data:\\n\" + str(train_data[j]))\n",
    "            print(\"Forward pass:\\n\" + str(network.full_forward_propagation(train_data[j])))\n",
    "            print(\"Target Data:\\n\" + str(train_data[j+1]))\n",
    "            #network.back_propagation_through_time(train_data[j], train_data[j+1])\n",
    "        else:\n",
    "            network.full_forward_propagation(train_data[j])\n",
    "            #network.back_propagation_through_time(train_data[j], train_data[j+1])\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Model Training Completed Successfully !!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :\n",
      " [[-0.00619986]\n",
      " [-0.37720284]\n",
      " [ 0.6463399 ]] \n",
      " ----------------\n",
      " [[0.86187742]\n",
      " [0.10832696]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.78086656]\n",
      " [-0.32564749]\n",
      " [ 0.66764041]] \n",
      " ----------------\n",
      " [[0.05019719]\n",
      " [0.99568981]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.41540763]\n",
      " [0.65871535]\n",
      " [0.78331597]] \n",
      " ----------------\n",
      " [[0.00845399]\n",
      " [1.03813781]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[ 0.24722475]\n",
      " [-0.24805785]\n",
      " [ 0.53004415]] \n",
      " ----------------\n",
      " [[0.91786636]\n",
      " [0.07840574]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.70279271]\n",
      " [ 0.46321518]\n",
      " [ 0.61447053]] \n",
      " ----------------\n",
      " [[0.02247825]\n",
      " [1.0280764 ]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.17676664]\n",
      " [ 0.2792578 ]\n",
      " [ 0.2540299 ]] \n",
      " ----------------\n",
      " [[0.92570571]\n",
      " [0.07711444]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.07945405]\n",
      " [ 0.11611852]\n",
      " [ 0.58647246]] \n",
      " ----------------\n",
      " [[0.92530286]\n",
      " [0.07696259]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[ 0.42428731]\n",
      " [-0.80201862]\n",
      " [ 0.86485241]] \n",
      " ----------------\n",
      " [[-0.00842461]\n",
      " [ 1.03266868]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.58921302]\n",
      " [0.99663906]\n",
      " [0.38109458]] \n",
      " ----------------\n",
      " [[0.00563894]\n",
      " [1.03932788]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.9959181 ]\n",
      " [0.52394166]\n",
      " [0.80544732]] \n",
      " ----------------\n",
      " [[-0.02351763]\n",
      " [ 1.00170404]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.89598111]\n",
      " [0.30033887]\n",
      " [0.06917741]] \n",
      " ----------------\n",
      " [[-0.01679617]\n",
      " [ 1.02353098]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.23299178]\n",
      " [ 0.1817426 ]\n",
      " [ 0.85213741]] \n",
      " ----------------\n",
      " [[0.55650583]\n",
      " [0.35144367]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.36102798]\n",
      " [0.13203824]\n",
      " [0.84910971]] \n",
      " ----------------\n",
      " [[0.86649165]\n",
      " [0.10585228]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.57679619]\n",
      " [ 0.94919995]\n",
      " [ 0.31126809]] \n",
      " ----------------\n",
      " [[-0.02502604]\n",
      " [ 0.9943119 ]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.29478427]\n",
      " [0.98395435]\n",
      " [0.52029562]] \n",
      " ----------------\n",
      " [[0.00129174]\n",
      " [1.04013317]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.91626869]\n",
      " [0.82537921]\n",
      " [0.40565307]] \n",
      " ----------------\n",
      " [[-0.00432107]\n",
      " [ 1.03903566]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.2812659 ]\n",
      " [0.99565242]\n",
      " [0.49594973]] \n",
      " ----------------\n",
      " [[0.00119708]\n",
      " [1.04013859]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.59998733]\n",
      " [0.76773847]\n",
      " [0.86732206]] \n",
      " ----------------\n",
      " [[-0.00184795]\n",
      " [ 1.03979696]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[-0.9030003 ]\n",
      " [ 0.44789778]\n",
      " [ 0.27488716]] \n",
      " ----------------\n",
      " [[0.07280535]\n",
      " [0.96463864]] \n",
      "--------------------------------\n",
      "Prediction :\n",
      " [[0.37148568]\n",
      " [0.50592229]\n",
      " [0.2266818 ]] \n",
      " ----------------\n",
      " [[0.41531909]\n",
      " [0.49951002]] \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_data = np.empty((0, 3, 1))\n",
    "while len(test_data) < 20:\n",
    "    X_loc = random.random()*2-1\n",
    "    Y_loc = random.random()*2-1\n",
    "    radius = random.random()\n",
    "    test_data = np.append(test_data, [[[X_loc], [Y_loc], [radius]]], axis=0)\n",
    "for x in test_data:\n",
    "    print(\"Prediction :\\n {} \\n ----------------\\n\".format(x), network.predict(x),\"\\n--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
