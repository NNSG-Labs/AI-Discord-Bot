{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](dcdw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random, math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def swish(x):\n",
    "    return x*sigmoid(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def activation_function(z,act):\n",
    "    if act==\"sigmoid\":\n",
    "        return sigmoid(z)\n",
    "    elif act == \"swish\":\n",
    "        return swish(z)\n",
    "    elif act == \"relu\":\n",
    "        return relu(z)\n",
    "    elif act==\"tanh\":\n",
    "        return np.tanh(z)\n",
    "    \n",
    "class Layer():\n",
    "    def __init__(self,input_size,output_size,act=None):\n",
    "        self.size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = act\n",
    "        self.weights = np.random.uniform(-1,1,(self.output_size,self.size))    \n",
    "        self.bias = np.random.uniform(-1,1,(output_size,1))\n",
    "        self.A = np.random.uniform(-1,1,(output_size,1))\n",
    "        self.Z = np.random.uniform(-1,1,(output_size,1))\n",
    "        \n",
    "    def forward_propagation(self, input_data):\n",
    "        self.Z = np.dot(self.weights,input_data)+self.bias\n",
    "        self.A = activation_function(self.Z, self.activation)\n",
    "        \n",
    "    def derMSE(self, target):\n",
    "        return 2*(self.A - target)\n",
    "    \n",
    "    def descent(self, input_data, gradient, learningRate):\n",
    "        \n",
    "        if(self.activation == \"tanh\"):\n",
    "            derZ = 1 - np.power(self.A, 2)\n",
    "        elif(self.activation == \"swish\"):\n",
    "            derZ = swish(self.Z) + sigmoid(self.Z) * (1 - swish(self.Z))\n",
    "        reps = (self.weights.shape[0], 1)\n",
    "        derWeights = np.tile(input_data.transpose(), reps)        # Size -> a(L-1)*a(L)\n",
    "        \"\"\"derWeights is a matrix with derivatives of Z WRT weights, which is transposed inputs, \n",
    "        repeated in rows n-times, where n is number of neurons.\n",
    "        \n",
    "    Example:\n",
    "    \n",
    "        input_data = [\n",
    "            [2],\n",
    "            [1],\n",
    "            [0]\n",
    "        ]\n",
    "        \n",
    "        derWeights = [\n",
    "            [2, 1, 0],\n",
    "            [2, 1, 0],\n",
    "            [2, 1, 0],\n",
    "            ... n-rows\n",
    "        ]\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        \"\"\" The below part adds the gradient to the derivative of A WRT input_data and passes this new\n",
    "        gradient through return, to be used as the gradient for next layer's descent.\n",
    "            dA/dX is made with 2 steps: adding the backprop gradient to the derivative of A WRT Z and then adding\n",
    "        the derivative of Z WRT input_data (chain rule).\n",
    "        \n",
    "        1.\n",
    "        Since A is a matrix shaped Nx1, where N is the number of outputs, the receiving gradient from the upper layer\n",
    "        must be the same shape. Therefore we can multiply the gradient and the derivative together element-wise.\n",
    "        \n",
    "    Example:\n",
    "        \n",
    "         derZ = [3x1]\n",
    "         gradient = [3x1]\n",
    "         firstGrad = [3x1] *(elementwise) [3x1] = [3x1]\n",
    "        \n",
    "        \"\"\"\n",
    "        firstGrad = np.multiply(gradient, derZ)             # ∂C/∂a(L) * ∂(act)/∂Z    a(L)*1 * a(L)*1 = a(L)*1\n",
    "        \n",
    "        \"\"\"\n",
    "        2.\n",
    "        What's left is adding the gradient of Z WRT input_data. \n",
    "        \n",
    "        This turns out to be the weights matrix. Now we have to multiply the firstGrad gradient to these weights \n",
    "        element-wise but since the firstGrad is Nx1 shape and the weights are NxM, where M are the features, \n",
    "        we need to reshape the gradient matrix to match the weights matrix by cloning gradient's columns:\n",
    "        \"\"\"\n",
    "        secondGrad = np.tile(firstGrad, (1, self.weights.shape[1]))          #Size -> a(L)*1 -> a(L)*a(L-1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Finally we multiply (E-W) secondGrad to the weights matrix:\n",
    "        \"\"\"\n",
    "        derX = np.multiply(self.weights,secondGrad)         #Size -> a(L)*a(L-1) * a(L)*a(L-1) = a(L)*a(L-1)\n",
    "        \n",
    "        \"\"\"But because same inputs are multiplied with many weights, we can sum those weights together. It turns out\n",
    "        that we can sum columns to do that\"\"\"\n",
    "        \n",
    "        derX = np.sum(derX, axis=0, keepdims=True)       #Size -> 1*a(L-1) \n",
    "        derBias = 1\n",
    "        \n",
    "        weightGrad = np.multiply(derWeights, np.tile(firstGrad, (1, self.weights.shape[1])))\n",
    "        self.weights = self.weights - learningRate * weightGrad\n",
    "        self.bias = self.bias - learningRate * firstGrad\n",
    "        \n",
    "        \"\"\"We return transposed matrix, because we desire inputs with a shape of Nx1 and right now finalGrad is \n",
    "        transposed\"\"\"\n",
    "        return derX.transpose()     # Size -> a(L-1)*1\n",
    "    \n",
    "        \n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "        self.epochs=10\n",
    "        self.learning_rate = 0.008\n",
    "    \n",
    "    def add_layer(self,input_size,output_size,activation=None):\n",
    "        new_layer = Layer(input_size,output_size,activation)\n",
    "        self.layers.append(new_layer)\n",
    "        \n",
    "    def forward_propagation(self,layer_no):\n",
    "        current_layer = self.layers[layer_no-1]\n",
    "        prev_layer = self.layers[layer_no-2]\n",
    "        act = current_layer.activation\n",
    "        input_data = prev_layer.A\n",
    "        self.Z = np.dot(weights,input_data)+self.bias\n",
    "        result = activation_function(self.Z,act)    # array containing neuron values\n",
    "        current_layer.A = result            #After forward propogation, fills in the neurons in that layer\n",
    "        return result\n",
    "    \n",
    "    def full_forward_propagation(self, input_data):\n",
    "        #print(\"layer 0 forward_propagation\")\n",
    "        \n",
    "        self.layers[0].forward_propagation(input_data)        # From input data to first layer\n",
    "        for i in range(1, len(self.layers)):\n",
    "            #print(\"layer \" + str(i) + \" forward_propagation\")\n",
    "            \n",
    "            self.layers[i].forward_propagation(self.layers[i-1].A)      #From layer i-1 to layer i \n",
    "        return self.layers[len(self.layers)-1].A\n",
    "    \n",
    "    def back_propagation(self, input_data, target):\n",
    "        gradient = self.layers[len(self.layers)-1].derMSE(target)     # ∂C/∂a(L)   Size -> a(L)*1\n",
    "        for i in range(0, len(self.layers)-1):\n",
    "            index = len(self.layers)-1 - i\n",
    "            #print(\"Layer \" + str(index) + \" backpropagation\")\n",
    "            gradient = self.layers[index].descent(self.layers[index-1].A, gradient, self.learning_rate)   #a(L)*1\n",
    "        self.layers[0].descent(input_data, gradient, self.learning_rate)\n",
    "            \n",
    "            \n",
    "    def predict(self,test_data):\n",
    "        self.layers[0].forward_propagation(test_data)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].forward_propagation(self.layers[i-1].A)\n",
    "        return self.layers[len(self.layers)-1].A\n",
    "\n",
    "\n",
    "\"\"\"train_data = np.array([[[3],[2],[1]],\n",
    "                      [[5],[1],[1]],\n",
    "                      [[5],[0],[0]],\n",
    "                      [[3],[0],[1]],\n",
    "                      [[4],[-1],[2]],\n",
    "                      [[3],[-1],[5]],\n",
    "                      [[3],[-1],[11]],\n",
    "                      [[3],[1],[3]],\n",
    "                      [[-1],[1],[1]],\n",
    "                      [[2],[0],[1]],\n",
    "                      [[0],[0],[1]],\n",
    "                      [[3],[0],[5]],\n",
    "                      [[2],[-1],[6]],\n",
    "                      [[1],[1],[-4]],\n",
    "                      [[0],[0],[-1]],\n",
    "                      [[2],[1],[-4]],\n",
    "                      [[2],[1],[-1]],\n",
    "                      [[1],[0],[-1]],\n",
    "                      [[10],[4],[-2]]])\"\"\"\n",
    "\n",
    "\n",
    "hoop_radius = 0.6\n",
    "plot_x =[]\n",
    "plot_y =[]\n",
    "plot_r = []\n",
    "plot_color = []\n",
    "give_true = True\n",
    "train_data = np.empty((0, 3, 1))\n",
    "while len(train_data) < 1000:\n",
    "    X_loc = random.random()*2-1\n",
    "    Y_loc = random.random()*2-1\n",
    "    radius = random.random()\n",
    "    \n",
    "    distance = math.sqrt(X_loc*X_loc + Y_loc*Y_loc) + radius\n",
    "    \n",
    "    if distance <= hoop_radius and give_true:\n",
    "        train_data = np.append(train_data, [[[X_loc], [Y_loc], [radius]]], axis=0)\n",
    "        give_true = False\n",
    "        plot_x.append(X_loc)\n",
    "        plot_y.append(Y_loc)\n",
    "        plot_r.append(radius*25600)\n",
    "        plot_color.append(2)\n",
    "    elif not give_true and distance > hoop_radius:\n",
    "        train_data = np.append(train_data, [[[X_loc], [Y_loc], [radius]]], axis=0)\n",
    "        give_true = True\n",
    "        plot_x.append(X_loc)\n",
    "        plot_y.append(Y_loc)\n",
    "        plot_r.append(radius*25600)\n",
    "        plot_color.append(0.5)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(plot_x, plot_y, s=plot_r, c=plot_color, edgecolor='black', alpha=0.8)\n",
    "plt.show()\n",
    "\"\"\"Here we generate target_data set using formula 2*x0 + -4*x1 + -x2\"\"\"\n",
    "target_data = []\n",
    "for i in range(0, len(train_data)):\n",
    "    if i%2 == 0:\n",
    "        target_data.append([[1], [0]])\n",
    "    else:\n",
    "        target_data.append([[0], [1]])\n",
    "\n",
    "        \n",
    "network = NeuralNetwork()\n",
    "network.add_layer(3, 3, \"tanh\")\n",
    "network.add_layer(3, 2, \"tanh\")\n",
    "network.add_layer(2, 2, \"swish\")\n",
    "\n",
    "for i in range(0, 800):\n",
    "    for j in range(0,len(train_data)):\n",
    "        if i%95==0 and j%19 == 0:\n",
    "            print(\"----------------\")\n",
    "            print(\"Input_data:\\n\" + str(train_data[j]))\n",
    "            print(\"Forward pass:\\n\" + str(network.full_forward_propagation(train_data[j])))\n",
    "            print(\"Target Data:\\n\" + str(target_data[j]))\n",
    "            network.back_propagation(train_data[j], target_data[j])\n",
    "        else:\n",
    "            network.full_forward_propagation(train_data[j])\n",
    "            network.back_propagation(train_data[j], target_data[j])\n",
    "            \n",
    "print(\"Prediction : \", network.predict([[0],[0],[0.5]]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
